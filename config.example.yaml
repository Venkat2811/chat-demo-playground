provider:
  base_url: http://127.0.0.1:8000/v1
  model: your-model-name
  kind: openai-chat   # or: openai-completions
  api_key: null
  force_output_tokens: false  # set true for vLLM servers to enforce exact output token length

datasets:
  - name: sharegpt_1k
    path: /absolute/path/to/sharegpt_1k.jsonl
    fmt: jsonl
    prompt_field: prompt          # field name containing input text
    token_len_field: prompt_len   # optional, if present it will be used
